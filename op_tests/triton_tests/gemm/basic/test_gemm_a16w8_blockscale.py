# SPDX-License-Identifier: MIT
# Copyright (C) 2024-2025, Advanced Micro Devices, Inc. All rights reserved.

import torch
import triton
import pytest
from aiter.ops.triton.gemm.basic.gemm_a16w8_blockscale import gemm_a16w8_blockscale
from aiter.ops.triton.utils.types import get_fp8_dtypes
from aiter.ops.triton.utils.types import str_to_torch_dtype

# from op_tests.triton_tests.test_fused_fp8_quant import per_token_fp8_group_quant
import torch.nn.functional as F


block_shape = (128, 128)


def run_torch(x, weight, w_scale, dtype=torch.bfloat16):
    block_shape_n, block_shape_k = block_shape
    m, k = x.shape
    n = weight.shape[0]
    scale_n = (n + block_shape_n - 1) // block_shape_n
    scale_k = (k + block_shape_k - 1) // block_shape_k

    # the pre-quant version now has accuracy issues
    # x, x_scale = per_token_fp8_group_quant(x, weight.dtype, block_shape_k)
    # x_scale = x_scale.repeat_interleave(block_shape_k, dim=1)
    # x = x.to(x_scale.dtype) * x_scale[:m, :k]
    # x = x.view(m, k)

    w_scale = w_scale.repeat_interleave(block_shape_n, dim=0)
    w_scale = w_scale.repeat_interleave(block_shape_k, dim=1)
    weight = weight.to(w_scale.dtype) * w_scale[:n, :k]

    out = F.linear(x.to(torch.float32), weight.to(torch.float32))

    return out.to(dtype)


def run_triton(x, weight, w_scale, dtype=torch.bfloat16, y=None):
    return gemm_a16w8_blockscale(x, weight, w_scale, dtype, y, pre_quant=False)


e5m2_type, e4m3_type = get_fp8_dtypes()


def get_x_vals():

    x_vals = [(1024 * v, 1024 * v, 1024 * v) for v in range(1, 9)]
    x_vals += [(4864, 4096, 8192), (9728, 8192, 65536)]
    x_vals += [
        (1, 1280, 8192),
        (32, 1280, 8192),
        (64, 1280, 8192),
        (128, 1280, 8192),
        (192, 1280, 8192),
        (256, 1280, 8192),
        (320, 1280, 8192),
        (512, 1280, 8192),
        (1024, 1280, 8192),
        (2048, 1280, 8192),
        (4096, 1280, 8192),
        (8192, 1280, 8192),
        (16384, 1280, 8192),
        (1, 8192, 1024),
        (32, 8192, 1024),
        (64, 8192, 1024),
        (128, 8192, 1024),
        (192, 8192, 1024),
        (256, 8192, 1024),
        (320, 8192, 1024),
        (512, 8192, 1024),
        (1024, 8192, 1024),
        (2048, 8192, 1024),
        (4096, 8192, 1024),
        (8192, 8192, 1024),
        (16384, 8192, 1024),
        (2048, 2048, 2049),
        (159, 17389, 597),
        (16, 576, 7168),
    ]
    x_vals += [
        (256, 8192, 1024),
        (256, 1024, 8192),
        (256, 32768, 8192),
        (256, 8192, 32768),
    ]
    # x_vals += [(1, 1, 1)]  # minimal case
    return x_vals


def generate_gemm_a16w8_blockscale_inputs(
    M: int,
    N: int,
    K: int,
    block_shape_n: int,
    block_shape_k: int,
    dtype=torch.bfloat16,
    layout: str = "TN",
    output=False,
):
    """
    The GEMM kernel expects:
    - x: (M, K) -> row-major format
    - w: (N, K) -> column-major format
    """
    scale_n = (N + block_shape_n - 1) // block_shape_n
    scale_k = (K + block_shape_k - 1) // block_shape_k

    if layout[0] == "T":
        x = torch.randn((M, K), dtype=torch.bfloat16).cuda() / 10
    else:
        x = torch.randn((K, M), dtype=torch.bfloat16).cuda().T / 10

    if layout[1] == "N":
        weight = (torch.rand((N, K), dtype=torch.float16, device="cuda") / 10).to(
            e4m3_type
        )
    else:
        weight = (
            (torch.rand((K, N), dtype=torch.float16, device="cuda") / 10)
            .to(e4m3_type)
            .T
        )

    w_scale = torch.rand([scale_n, scale_k], dtype=torch.float32, device="cuda")

    y = None
    if output:
        y = torch.empty((M, N), dtype=dtype, device="cuda").cuda()

    return x, weight, w_scale, y


@pytest.mark.parametrize(
    "dtype, M, N, K, output",
    [
        (dtype, *shape, output)
        for output in [True, False]
        for dtype in ["bf16"]
        for shape in get_x_vals()
    ],
)
def test_gemm(dtype, M, N, K, output):
    block_shape_n, block_shape_k = block_shape

    dtype = str_to_torch_dtype[dtype]
    x, weight, w_scale, y = generate_gemm_a16w8_blockscale_inputs(
        M,
        N,
        K,
        block_shape_n,
        block_shape_k,
        dtype=dtype,
        output=output,
    )

    a = run_torch(x, weight, w_scale, dtype)
    b = run_triton(x, weight, w_scale, dtype, y)

    triton.testing.assert_close(a, b, atol=0.1, rtol=0.1)
